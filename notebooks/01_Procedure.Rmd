---
title: "Productivity Prediction"
author: "Alvaro Manzanas"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  github_document:
    md_extensions: +autolink_bare_uris+tex_math_single_backslash
    fig_width: 12
    fig_height: 8
    df_print: kable
    number_sections: true
    toc: true
    toc_depth: 3
  rmdformats::html_clean:
    code_folding: hide
    self_contained: true
    number_sections: true
    thumbnails: false
    lightbox: true
    highlight: breezedark
subtitle: "Analysis Procedure"
csl: apa.csl
pkgdown:
  as_is: true
editor_options:
  markdown:
    wrap: 79
---
```{r global_options, include=FALSE}
knitr::opts_chunk$set(
    warning = FALSE,
    fig.align = "center",
    fig.width = 12,
    fig.height = 8,
    out.width = "100%",
    results = "hold",
    collapse = TRUE,
    message = FALSE,
    warning = FALSE
)
```
```{r results='hide'}
library(data.table)     # to read dataset
library(tidyverse)      # data wrangling and ggplot2
library(patchwork)      # a ggplot extension
library(ggstatsplot)    # a ggplot extension
library(GGally)         # a ggplot extension
library(ggpubr)         # a ggplot extension
library(skimr)          # descriptive analysis
library(car)            # Statistical Analysis
library(Metrics)        # Evaluation Metrics for Machine Learning
library(randomForest)   # Random Forest
library(caret)          # various predictive analysis
library(ROCR)           # ROC curve
library(Cubist)         # Cubist Model Tree
library(rpart)          # Regression Trees
library(rpart.plot)     # Tree plot for rpart
library(psych)          # Principal Components Analysis
library(leaps)          # Linear regression 
library(kernlab)        # Support Vector Machines
```
# Loading Data
```{r eval=FALSE, include=FALSE}
rm(list = ls(all.names = TRUE))
```

```{r}
library(data.table)

df <- fread(file = "../data/garments_worker_productivity.csv")
df <- as.data.frame(df, stringsAsFactor = FALSE)
df.original <- df
head(df)
```

## Key Variable

```{r results='hide'}
library(tidyverse)

vec.key <- c(paste(sprintf("%04.0f", seq(1:nrow(df)) ),
                   sprintf("%02.0f", round(df$team, 0)),
                   sep="_") )

df <- bind_cols(vec.key, df)
remove(vec.key)

colnames(df)[1] <- c("id")
```

## Column Classes
```{r}
glimpse(df)
```

```{r results='hide'}
    ## Change date and team classes
df <- df %>% 
    mutate( team = print(sprintf("%02.0f", team)) ) %>%
    mutate_at( "date", as.Date, format = "%m/%d/%Y")

    ## Round no_of_workers and changing class
df <- df %>%
    mutate( no_of_workers = ceiling(no_of_workers) ) %>%
    mutate_at( "no_of_workers", as.integer)

table(df$targeted_productivity) 
    # there is a value '0.07' which is unlikely to
    # be correct, could be a typographical error for '0.70' 
df <- df %>% mutate( 
    targeted_productivity = ifelse(targeted_productivity==0.07, 
                                   0.70, targeted_productivity) )
```

# Data Cleaning

```{r results='hide'}
# Enabling 'just in time' compilation
compiler::enableJIT(3)

# Changing scientific notion
options( scipen = 999, digits = 4 )
```

## Duplicates and Missing Values

```{r}
# Check duplicates
anyDuplicated(df[,2:ncol(df)])

# Check Missing Values
anyNA(df)
```

```{r}
for (var in colnames(df)){
    print(
        paste(var, sum(is.na(df[var]) ) )
    )
}
```

```{r}
df.na <- df[which(is.na(df$wip)),]

df.na %>% select(team, department, date, wip, actual_productivity) %>% .[11:20,]
table(df$department) # Same number of rows on department-finishing

    ## Answering: department == finishing, means wip == NA ?

df.dep.filt <- df %>% filter(department == "finishing")
all.equal( df.dep.filt$id, df.na$id ) # TRUE
remove(df.dep.filt)
remove(df.na)
```

```{r}
# filling NA values with 0 (if the team finish, then no wip)

df.clean <- df %>% replace_na( list( wip = 0) )
anyNA(df.clean) # FALSE
```

## Unique values for characters/factors

```{r}
# Checking unique values

for (var in colnames(df.clean[,-1])){
    if( is.character(df.clean[, var])){
        print( paste(var, unique(df.clean[, var]) ) )
    }
} # There are 5 Quarters.

```
```{r}
filt.quarter <- df.clean %>% 
    select(id, date, quarter) %>% 
    filter(quarter == "Quarter5") 
# Days 29, 30 and 31 are labeled as Quarter5, should be Quarter4
slice_sample(filt.quarter, n=10)
remove(filt.quarter)
```

```{r}
df.clean <- df.clean %>% 
    mutate(
        quarter = replace(quarter, quarter == "Quarter5", "Quarter4")
    )
```

# Exploration

```{r}
# Exploring the data with histograms
for (var in colnames(select_if(df.clean[,1:ncol(df.clean)], is.numeric) ) ) {
    hist( unlist( df.clean[,var]), col = "blue",
          main = paste("Histogram of", var),
          xlab = var)
}   # Target_productivity and actual_productivity have left tail
    # and actual_productivity is close to normal distribution
    # The rest of variables have right tail
    # except no_of_worker which seems to be mesokurtic
```

## Creating target variable

```{r}
# Creating a target variable for productivity
df.clean <- df.clean %>% mutate( 
    productivity_cat = ifelse(actual_productivity >= targeted_productivity,
                              "achieved", "unsuccess") )
df.clean$productivity_cat <- as.factor(df.clean$productivity_cat)

```

```{r}
summ <- skim(df.clean)
summ %>% 
    filter(skim_type == "numeric") %>%
    mutate( cv = numeric.sd / numeric.mean )
    # For incentive, idle_time, idle_men, no_of_style_change the p75 are near 0 or 0
        # also this variables have high CV (coefficient of variation)
    # Most of variables have high standard deviation which suggest outliers and
        # spread data
    # 875 (73.1%) achieve the target productivity 

table( df.clean$productivity_cat)
```

## Boxplots

```{r}
    ## Boxplots

df.clean %>%
    ggplot() +
    geom_boxplot( aes( x = actual_productivity,
                       y = productivity_cat) ) +
    labs( y = "") +
    theme_classic() +
    theme(legend.position = "none")


df.clean %>%
    ggplot() +
    geom_boxplot( aes( x = incentive,
                       y = productivity_cat) ) +
    labs( y = "") +
    theme_classic() +
    theme(legend.position = "none")


g1 <- df.clean %>%
    filter( productivity_cat == "achieved") %>%
    ggplot() +
    geom_boxplot( aes( y = reorder(team, actual_productivity),
                       x = actual_productivity,
                       fill = productivity_cat ) ) +
    labs( title = paste( "Productivity is achieved"),
          subtitle = "On Team levels",
          y = "") +
    theme_classic() +
    theme(legend.position = "none")
g2 <- df.clean %>%
    filter( productivity_cat == "unsuccessful") %>%
    ggplot() +
    geom_boxplot( aes( y = reorder(team, actual_productivity),
                       x = actual_productivity,
                       fill = productivity_cat ) ) +
    labs( title = paste( "Productivity is unsuccessful"),
          subtitle = "On Team levels",
          y = "") +
    theme_classic() +
    theme(legend.position = "none")

(g1|g2) # It can be seen differences between both groups
        # 'unsuccessful' is mode skewed but 'achieved' has outliers

```


```{r}
df.achieve <- df.clean %>% filter(productivity_cat == "achieved")
df.unsucces <- df.clean %>% filter(productivity_cat == "unsuccess")


g1 <- df.achieve %>%
    ggplot() +
        geom_boxplot( aes( y = reorder(targeted_productivity, actual_productivity),
                           x = actual_productivity,
                           fill = productivity_cat ) ) +
        labs( title = paste( "Productivity is achieved"),
              subtitle = "On targeted_productivity levels",
              y = "") +
        theme_classic() +
        theme(legend.position = "none")
g2 <- df.unsucces %>%
    ggplot() +
        geom_boxplot( aes( y = reorder(targeted_productivity, actual_productivity),
                           x = actual_productivity,
                           fill = productivity_cat ) ) +
        labs( title = paste( "Productivity is unsuccessful"),
              subtitle = "On targeted_productivity levels",
              y = "") +
        theme_classic() +
        theme(legend.position = "none")

(g1|g2)

df.achieve %>%
    ggplot() +
    geom_boxplot( aes( y = reorder(department, actual_productivity),
                       x = actual_productivity,
                       fill = productivity_cat ) ) +
    labs( title = paste( "Productivity is achieved"),
          subtitle = "On targeted_productivity levels",
          y = "") +
    theme_classic() +
    theme(legend.position = "none")

```

### Multiple Comparisons

```{r}

# Comparisons between groups on every numeric variable

for (var in colnames(select_if(df.clean[,1:ncol(df.clean)], is.numeric) ) ) {
    wilx <- wilcox.test(df.unsucces[[var]], df.achieve[[var]])
    rbis <- abs(sum(-1, (2 * wilx$statistic) / (nrow(df.achieve) * nrow(df.unsucces) ) ) )
    cat("\n===============\n",
        "\nMann-Whitney U between spam and not spam on:", var, 
        "\np-value:", sprintf("%6.4f", wilx$p.value),
        "\nr:", rbis)
}   # Variables smv, wip, over_time, incentive, no_of_workers, actual_productivity
    # will be interesting to the model


```

## Correlations

```{r}

df.clean %>% 
    select(smv, wip, over_time, incentive, no_of_workers, actual_productivity) %>% 
    cor()
```


## Team Comparisons

```{r}
df.clean %>% ggbarstats( y = team, x = productivity_cat, 
                         title = "Productivity achieve/unsuccess proportions by team")
    # Teams 03, 01, 06, and 04 have the highest achieve rate (>80%)
    # Teams 08, 07, 06, and 09 have a lack of achieve productivity (33-45% unsuccess)
        # Also, 07 and 08 teams have no significance, thus suggests that productivity 
        # in these two teams is not being significantly influenced by the variable “work team”.
    # Chi-square suggests that there is a relationship between productivity and teamwork in teams with p < 0.05.
```

```{r}
df.clean <- df.clean %>%
    mutate( unproductive = ifelse(team == "07" | team == "08", "yes", "no") ) 
df.clean$unproductive <- as.factor(df.clean$unproductive)
```

```{r warning=FALSE}
ggboxplot( data = df.clean, y = "actual_productivity", x = "unproductive",
           color = "unproductive", add = "jitter", shape = "unproductive") +
    stat_compare_means( comparisons = c("yes", "no") ) +
    stat_compare_means( label.y = 1.2)

( g1 <- ggboxplot( data = df.clean, y = "incentive", x = "unproductive",
           color = "unproductive", add = "jitter", shape = "unproductive",
           ylim = c(0,150)) +
    stat_compare_means( comparisons = c("yes", "no") ) +
    stat_compare_means( label.y = 140, size = 5) +
    labs( title = "Incentive by teams on productiveness",
          subtitle = "The lower inventive for the teams, the lower the productivity ('unsuccess')") +
    theme(
        plot.title = element_text(size = 16), 
        plot.subtitle = element_text(size = 13), 
        axis.text = element_text(size = 12), 
        legend.text = element_text(size = 12), 
        legend.title = element_text(size = 14), 
        axis.title = element_text(size = 14), 
    ) )

    # These results suggest that those teams (07 and 08) that have been observed
    # to be less productive by not meeting target productivity 
    # have significantly lower incentives.

ggboxplot( data = df.clean, y = "over_time", x = "unproductive",
           color = "unproductive", add = "jitter", shape = "unproductive",
           ylim = c(0,10000)) +
    stat_compare_means( comparisons = c("yes", "no") ) +
    stat_compare_means( label.y = 8000, label.x = "yes")

ggboxplot( data = df.clean, y = "no_of_workers", x = "unproductive",
           color = "unproductive", add = "jitter", shape = "unproductive",
           ylim = c(0,80)) +
    stat_compare_means( comparisons = c("yes", "no") ) +
    stat_compare_means( label.y = 75)

( g2 <- ggboxplot( data = df.clean, y = "no_of_style_change", x = "unproductive",
           color = "unproductive", add = "jitter", shape = "unproductive",
           ylim = c(0,2)) +
    stat_compare_means( comparisons = c("yes", "no") ) +
    stat_compare_means( label.y = 2, size = 5 ) +
    labs( title = "Number Style Changes by teams on productiveness",
          subtitle = "With more number of changes there are\nless target productivity 'achieve'") +
    theme(
        plot.title = element_text(size = 16), #
        plot.subtitle = element_text(size = 13), #
        axis.text = element_text(size = 12), #
        legend.text = element_text(size = 12), #
        legend.title = element_text(size = 14), #
        axis.title = element_text(size = 14), #
    ) )
```

```{r}

wilcox.test( df.clean$incentive ~ df.clean$unproductive )
df.clean %>% group_by(unproductive) %>%
    summarize(median_incentive = median(incentive), 
              mean_incentive = mean(incentive),
              sd_incentive = sd(incentive) )

wilcox.test( df.clean$no_of_style_change ~ df.clean$unproductive )
df.clean %>% group_by(unproductive) %>%
    summarize(median_incentive = median(no_of_style_change), 
              mean_incentive = mean(no_of_style_change),
              sd_incentive = sd(no_of_style_change) )

    # No significant differences in 'over_time' or 'no_of_workers' were observed 
    # comparing the teams. However, in `no_of_style_change' significant 
    # differences are observed, being the teams with the lowest productivity 
    # the ones with the highest style changes.

cor(df.clean$actual_productivity, df.clean$no_of_style_change)

    # An inverse relationship (r= -0.207) can be seen between 
    # current productivity and 'no_of_style_change', 
    # so the more style changes the lower the productivity.

```

```{r warning=FALSE}
(g1|g2)
```

# Modeling

## Classification

### Random Forest

```{r results='hide'}
# Selecting the best variables thanks to previous comparisons and plots
# and creating the data frames for modeling (classification)

df.model <- df.original %>% 
    select(department, team, targeted_productivity, smv, wip, over_time,
           incentive, no_of_workers, no_of_style_change, actual_productivity) %>%
    mutate( team = print(sprintf("%02.0f", team) ),
            no_of_workers = ceiling(no_of_workers),
            targeted_productivity = ifelse(targeted_productivity==0.07, 
                                           0.70, targeted_productivity)) %>%
    mutate( targeted_productivity = print(sprintf("%02.2f", targeted_productivity) ),
            productivity_cat = ifelse(actual_productivity < targeted_productivity,
                                      "unsuccess", "achieved") ) %>%
    replace_na( list( wip = 0) ) %>%
    mutate_if(is.character, as.factor)
```

#### Training

```{r}
    ## Splitting into 80/20

set.seed(69)

n.rows <- nrow(df.model)
idx <- createDataPartition(df.model$productivity_cat, p = 0.8, list = FALSE)

train.base <- df.model[idx,-c(10)]
test.base <- df.model[-idx,-c(10)]

    ## Testing proportions of target variable

cat("Achieved vs. Unsuccess:\n",
    "Original set:\n",
    prop.table( table(df.model$productivity_cat) ),
    "\n==================\n",
    "Train set:\n",
    prop.table( table(train.base$productivity_cat) ),
    "\n==================\n",
    "Test set:\n",
    prop.table( table(test.base$productivity_cat) ) )
```

#### Model Random Forest

```{r}
model.rf <- randomForest(productivity_cat ~ . - team - department, data = train.base,
                         importance = TRUE,
                         ntree = 212)
#saveRDS(model.rf, file = "scripts/models/model_random_forest.rds")
plot(model.rf)

varImpPlot(model.rf)
```

```{r}
        ### Evaluating the model

model.rf.predict <- predict( model.rf, test.base)
confusionMatrix( model.rf.predict, test.base$productivity_cat, positive = "achieved")
```

```{r}
        ### ROC Curve

rf.df.predict <- data.frame( predict( model.rf, test.base, type = 'prob') )
rf.roc <- prediction( rf.df.predict$unsuccess, test.base$productivity_cat)

roc <- performance(rf.roc, 'tpr', 'fpr')
auc <- performance(rf.roc, 'auc')
plot(roc, colorize = T, lwd = 2,
     main = "Roc curve",
     sub = paste( "AUC =", auc@y.values[[1]] ) )
abline(0.0, 1.0)
```

#### Model Monte-Carlo Random Forest

```{r}
train.Control <- trainControl( method = "repeatedcv",
                               number = 12,
                               repeats = 8,
                               search = "grid")

model.rf.mc <- train( productivity_cat ~ . - team - department,
                      data = train.base,
                      method = "rf",
                      tuneGrid = data.frame(mtry = 4),
                      ntree = 220,
                      trControl = train.Control)
#saveRDS(model.rf.mc, file = "scripts/models/model_monte_carlo_rf.rds")
print(model.rf.mc)
```

```{r}
        ### Evaluating the model
        
model.rf.mc.predict <- predict( model.rf.mc, test.base)
confusion_matrix <- confusionMatrix( model.rf.mc.predict, test.base$productivity_cat, positive = "achieved")
confusion_matrix
```

```{r}
        ### ROC Curve

rf.df.predict <- data.frame( predict( model.rf.mc, test.base, type = 'prob') )
rf.roc <- prediction( rf.df.predict$unsuccess, test.base$productivity_cat)

roc <- performance(rf.roc, 'tpr', 'fpr')
auc <- performance(rf.roc, 'auc')
plot(roc, colorize = T, lwd = 2,
     main = "Roc curve. 'unsuccessful' as positive class",
     sub = paste( "AUC =", auc@y.values[[1]] ) )
abline(0.0, 1.0)
```

### Logistic Regression

```{r}
df.model.bin <- df.model %>%
    mutate( across( where(is.numeric), ~ log(. + 1) ) ) %>%
    mutate( across( where(is.numeric), scale ) ) %>%
    mutate( actual_productivity = df.original$actual_productivity,
            productivity_cat = as.numeric(productivity_cat) ) %>%
    mutate( productivity_cat = ifelse(productivity_cat == 2, 0, 1 ) )
```

```{r}
    ## Splitting into 80/20

set.seed(69)

n.rows <- nrow(df.model.bin)
idx <- createDataPartition(df.model.bin$productivity_cat, p = 0.8, list = FALSE)

train.bin <- df.model.bin[idx,-c(10)]
test.bin <- df.model.bin[-idx,-c(10)]
```

```{r}
fit.bin <- glm(data = train.bin,
               productivity_cat ~ . - team - department,
               family = binomial(link="logit") )
summary(fit.bin)

# Suggestions:
    # When target productivity are 0.60, 0.70, and 0.80 the productivity are more
        # favorable
    # And 'incentive' variable has high impact
```

```{r}
# odd-ratio percentage, how likely is "achieve"
(exp(coefficients(fit.bin)) -1 ) *100
```

```{r}
bin.prob <- predict(fit.bin, test.bin,
                       type = "response") # 'response' for probabilities

# Confusion Matrix
table(Predicted = fit.bin$fitted.values > 0.5, Actual = train.bin$productivity_cat)
table(Predicted = fit.bin$fitted.values > 0.5, Actual = train.bin$productivity_cat) %>%
    prop.table() %>% addmargins()
```

```{r}
pred.labels <- ifelse(bin.prob > 0.5, 1, 0)
confusionMatrix(data = factor(pred.labels),
                reference = factor(test.bin$productivity_cat), 
                positive = "1")
```

### Support Vector Machines

```{r}
fit.svm <- ksvm(productivity_cat ~ . ,
                data = train.base,
                kernel = "vanilladot",
                prob.model = TRUE)

fit.svm
```

```{r}
pred.svm <- predict(fit.svm, test.base)

table(Predicted = pred.svm == "achieved", Actual = test.base$productivity_cat) %>%
    prop.table() %>% addmargins() 
```

```{r}
confusionMatrix(data = factor(pred.svm),
                reference = factor(test.base$productivity_cat), 
                positive = "achieved")
```

```{r}
rf.df.predict <- data.frame( predict( fit.svm, test.base, type = 'prob') )
rf.roc <- prediction( rf.df.predict$unsuccess, test.base$productivity_cat)

roc <- performance(rf.roc, 'tpr', 'fpr')
auc <- performance(rf.roc, 'auc')
plot(roc, colorize = T, lwd = 2,
     main = "Roc curve.",
     sub = paste( "AUC =", auc@y.values[[1]] ) )
abline(0.0, 1.0)
```

## Regressions

```{r results='hide'}
# Selecting the best variables thanks to previous comparisons and plots
# and creating the data frames for modeling (classification)

df.model.reg <- df.original %>% 
    mutate( team = print(sprintf("%02.0f", team) ),
            no_of_workers = ceiling(no_of_workers),
            targeted_productivity = ifelse(targeted_productivity==0.07, 
                                           0.70, targeted_productivity)) %>%
    mutate( targeted_productivity = print(sprintf("%02.2f",
                                                  targeted_productivity) ) ) %>%
    mutate( quarter = replace(quarter, quarter == "Quarter5", "Quarter4") ) %>%
    replace_na( list( wip = 0) ) %>%
    mutate_if(is.character, as.factor)
```

| lambda | -2 | -1 | -0.5 | 0 | 0.5 | 1 | 2 |
|---|---|---|---|---|---|---|---|
| Transformation| 1/*Y*^2 | 1/*Y* | 1/sqrt(*Y*) | log(*Y*) | sqrt(*Y*) | None | *Y*^2 |

```{r}
# Checking lambda to choose transformations
library(car)
for (var in colnames(select_if(df.model.reg, is.numeric))) {
    s <- summary(powerTransform(df.model.reg[[var]] + 1))
    cat("\n==========\nVariable:", var, "results:\n")
    print(s)
}

```

```{r}
# Data frame with logarithmic transformation and normalization
    # excluding -idle_time, -idle_men, -no_of_style_change

df.model.reg <- df.model.reg %>%
    mutate( across( where(is.numeric), ~ log(. + 1) ) ) %>%
    mutate( across( where(is.numeric), scale ) ) %>%
    mutate( actual_productivity = df.original$actual_productivity,
            idle_time = df.original$idle_time,
            idle_men = df.original$idle_men,
            no_of_style_change = df.original$no_of_style_change)
```

### Linear Regression Stepwhise

```{r}
## Splitting into 80/20

set.seed(69)

n.rows <- nrow(df.model.reg)
pct80 <- n.rows * 0.8
idx <- sample(n.rows, pct80)

train.reg <- df.model.reg[idx,]
test.reg <- df.model.reg[-idx,]
```

```{r}
## Training Linear Regression Stepwise

library(leaps)

leaps <- regsubsets(data = train.reg, 
                    actual_productivity ~ . - date 
                    - department - day - quarter
                    - idle_time - idle_men - team, nbest = 4,
                    nvmax = 50, really.big = T, 
                    )

subsTable <- function(obj, scale){
    x <- summary(leaps)
    m <- cbind(round(x[[scale]],3), x$which[,-1])
    colnames(m)[1] <- scale
    m[order(m[,1], decreasing = T), ]
}

subsTable(leaps, scale="adjr2")[1:6,]
```

```{r}
#
#   # Principal Components

library(psych)
train.reg %>% select_if(is.numeric) %>% .[,-15] %>%
fa.parallel(fa="pc", n.iter=100,
            show.legend=FALSE, main="Scree plot with parallel analysis")
abline(h=1)

principal(train.reg[,-c(1:6, 15)], nfactors = 3)
```

### Multiple Linear Regression

```{r}
# Regression with selected variables

sel.var <- c("smv", "wip", "over_time", "incentive", "no_of_workers",
             "team", 
             "no_of_style_change",
             "targeted_productivity", "actual_productivity")

train.reg %>%
    select("smv", "wip", "over_time", "incentive", "no_of_workers",
           "team", 
           "no_of_style_change",
           "targeted_productivity", "actual_productivity") %>% 
    select_if(is.numeric) %>%
    cor()

train.reg[,sel.var] %>% select_if(is.numeric) %>%
    scatterplotMatrix(smooth = FALSE, main = "Scatter Plot Matrix")

fit <- lm(actual_productivity ~ #smv + wip + 
              over_time + incentive +
              no_of_workers + no_of_style_change +
            #  team + 
              targeted_productivity,
          data = train.reg)
    # smv and wip excluded for Multicollinearity (VIF > 10)


summary(fit)
```

```{r}
pred.fit <- predict(fit, test.reg)

Metrics::rmse(test.reg$actual_productivity, pred.fit) # 0.15
Metrics::mae(test.reg$actual_productivity, pred.fit) # 0.109
Metrics::mse(test.reg$actual_productivity, pred.fit) # 0.023
R2(test.reg$actual_productivity, pred.fit) # 0.207

# Suggestions:
    # Intercept: Dependent Variable value when independent variables are 0
        # means DV when there are no effect of IV.
    # One standard deviation increase for log(incentive) will  
        # rise 0.399 of actual productivity
    # One standard deviation increase for log(no_of_workers) will 
        # rise 0.467 of actual productivity
    # Most relevant increases is when targeted_productivity are
        # at 0.65 or above, rising between 0.67 and 1.24
```

```{r}
confint(fit)

plot(fit)
# Suggestions:
    # Homoscedasticity (scale-location) and Linearity (Residuals vs Fitted)
        # are assumptions reached.
    # Normality it's been not met


```

```{r}
# Normality
qqPlot(fit, labels=row.names(states), id=list(method="identify"),
       simulate=TRUE, main="Q-Q Plot")

# Linearity
crPlots(fit)

# Homoscedasticity
spreadLevelPlot(fit)

```

```{r}

# Multicollinearity
vif(fit)
```

### Robust Linear Regression

```{r results='hide'}
# Selecting the best variables thanks to previous comparisons and plots
# and creating the data frames for modeling (classification)

df.model.reg <- df.original %>% 
    mutate( team = print(sprintf("%02.0f", team) ),
            no_of_workers = ceiling(no_of_workers),
            targeted_productivity = ifelse(targeted_productivity==0.07, 
                                           0.70, targeted_productivity)) %>%
    mutate( targeted_productivity = print(sprintf("%02.2f",
                                                  targeted_productivity) ) ) %>%
    mutate( quarter = replace(quarter, quarter == "Quarter5", "Quarter4") ) %>%
    replace_na( list( wip = 0) ) %>%
    mutate_if(is.character, as.factor)
```

```{r}
## Splitting into 80/20

set.seed(69)

n.rows <- nrow(df.model.reg)
pct80 <- n.rows * 0.8
idx <- sample(n.rows, pct80)

train.reg <- df.model.reg[idx,]
test.reg <- df.model.reg[-idx,]
```

```{r}
fit.rlm <- MASS::rlm(actual_productivity ~ #smv + wip + 
              over_time + incentive +
              no_of_workers + no_of_style_change +
              #team + 
                  targeted_productivity,
          data = train.reg)

summary(fit.rlm)
```

```{r}
fit.rlm.pred <- predict(fit.rlm, test.reg)

Metrics::rmse(test.reg$actual_productivity, fit.rlm.pred) # 0.149
Metrics::mae(test.reg$actual_productivity, fit.rlm.pred) # 0.106
Metrics::mse(test.reg$actual_productivity, fit.rlm.pred) # 0.022
R2(test.reg$actual_productivity, fit.rlm.pred) # 0.213
```

### Random Forest

```{r}
fit.rf <- randomForest(actual_productivity ~ #smv + wip + 
                 over_time + incentive +
                 no_of_workers + 
                 team + targeted_productivity,
             data = train.reg)

fit.rf

plot(fit.rf)

importance(fit.rf)

varImpPlot(fit.rf)

fit.rf.pred <- predict(fit.rf, test.reg)
```

```{r}
# RMSE:
sqrt(mean((fit.rf.pred - test.reg$actual_productivity)^2))
# Coefficient of Determination:
summary(lm(fit.rf.pred ~ test.reg$actual_productivity))$r.squared

Metrics::rmse(test.reg$actual_productivity, fit.rf.pred) # 0.1288
Metrics::mae(test.reg$actual_productivity, fit.rf.pred) # 0.08681
Metrics::mse(test.reg$actual_productivity, fit.rf.pred) # 0.01658
R2(test.reg$actual_productivity, fit.rf.pred) # 0.4193
```

### Regression Trees

```{r results='hide'}
df.model.rpart <- df.original %>%
    select(-date, -day, -quarter, -department) %>%
    mutate( team = print(sprintf("%02.0f", team) ),
            no_of_workers = ceiling(no_of_workers),
            targeted_productivity = ifelse(targeted_productivity == 0.07, 
                                           0.70, targeted_productivity)) %>%
    mutate( targeted_productivity = print(sprintf("%02.2f", 
                                                  targeted_productivity) ) ) %>%
    replace_na( list( wip = 0) ) %>%
    mutate_if(is.character, as.factor)
```

```{r}
# Train and test sets

set.seed(69)

n.rows <- nrow(df.model.rpart)
idx <- createDataPartition(df.model.rpart$actual_productivity, p = 0.8, list = FALSE)

train.rpart <- df.model.rpart[idx,-c(3,4,7,8)] # Excluding smv, wip, idle_
test.rpart <- df.model.rpart[-idx,-c(3,4,7,8)]
remove(n.rows)
```

```{r}
library(rpart)

fit.rpart2 <- rpart(actual_productivity ~ . -team, data = train.rpart)

fit.rpart2

```

```{r}
library(rpart.plot)
rpart.plot(fit.rpart2, digits=3, type = 1, tweak=1.4)

```


```{r}
pred.rpart2 <- predict(fit.rpart2, test.rpart)
summary(test.rpart$actual_productivity)
summary(pred.rpart2)

```

```{r}
MAE <- function(actual, predicted) {
    mean(abs(actual - predicted) )
}
MSE <- function(actual, predicted) {
    mean((actual - predicted)^2)
}
RMSE <- function(actual, predicted) {
    sqrt(mean((actual - predicted)^2))
}
R2 <- function(actual, predicted) {
    ss_res <- sum((actual - predicted)^2)
    ss_tot <- sum((actual - mean(actual))^2)
    1 - (ss_res / ss_tot)
}
```

```{r}
cor(pred.rpart2, test.rpart$actual_productivity) # 0.66
R2(test.rpart$actual_productivity, pred.rpart2)  # 0.44

MAE(test.rpart$actual_productivity, pred.rpart2) # 0.096

MSE(test.rpart$actual_productivity, pred.rpart2) # 0.019

RMSE(test.rpart$actual_productivity, pred.rpart2)# 0.137

```

### Cubist Model Tree

```{r}
library(Cubist)

    # Training set with log() and scale()

fit.cubist <- cubist( x = train.reg[-c(1,2,4,5,7,8,11,12,15)],
                      y = train.reg$actual_productivity,
                      rules = NA)

fit.cubist

summary(fit.cubist)


```

```{r}
pred.cubist <- predict(fit.cubist, test.reg)

R2(test.reg$actual_productivity, pred.cubist)   # 0.39

MAE(test.reg$actual_productivity, pred.cubist)  # 0.084

MSE(test.reg$actual_productivity, pred.cubist)  # 0.017

RMSE(test.reg$actual_productivity, pred.cubist) # 0.131
```




